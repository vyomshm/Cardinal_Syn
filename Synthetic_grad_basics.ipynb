{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate a synthetic dataset (har! har!) using binary addition. So, the network will take two, random binary numbers and predict their sum (also a binary number). \n",
    "\n",
    "### The nice thing is that this gives us the flexibility to increase the dimensionality (~difficulty) of the task as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_bits = 8, num_examples=10000):\n",
    "    def int2vec(x, n_bits= num_bits):\n",
    "        binary_rep=np.array(list(np.binary_repr(x))).astype('int')\n",
    "        out=np.zeros(n_bits)\n",
    "        out[-len(binary_rep):]= binary_rep\n",
    "        return out\n",
    "    \n",
    "    x_left_int = (np.random.rand(num_examples) * 2**(num_bits-1)).astype('int')\n",
    "    x_right_int = (np.random.rand(num_examples) * 2**(num_bits-1)).astype('int')\n",
    "    y_int = x_left_int + x_right_int\n",
    "    \n",
    "    x=list()\n",
    "    y=list()\n",
    "    for i in range(num_examples):\n",
    "        x.append(np.concatenate((int2vec(x_left_int[i]),int2vec(x_right_int[i]))))\n",
    "        y.append(int2vec(y_int[i]))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "bits = 12\n",
    "n_examples = 1000\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: two concatenated binary values:\n",
      "[0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1.]\n",
      "\n",
      "Output: binary value of their sum:\n",
      "[0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1.]\n",
      "\n",
      "Input shape : (1000, 24)\n",
      "\n",
      "Output shape : (1000, 12)\n"
     ]
    }
   ],
   "source": [
    "x,y = generate_dataset(num_bits = bits, num_examples = n_examples)\n",
    "print(\"Input: two concatenated binary values:\")\n",
    "print(x[0])\n",
    "print(\"\\nOutput: binary value of their sum:\")\n",
    "print(y[0])\n",
    "print('\\nInput shape :',x.shape)\n",
    "print('\\nOutput shape :', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "alpha = 0.1\n",
    "input_dim = len(x[0])\n",
    "layer_1_dim = 128\n",
    "layer_2_dim = 64\n",
    "output_dim = len(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_0_1 = (np.random.randn(input_dim,layer_1_dim) * 0.2) - 0.1\n",
    "weights_1_2 = (np.random.randn(layer_1_dim,layer_2_dim) * 0.2) - 0.1\n",
    "weights_2_3 = (np.random.randn(layer_2_dim,output_dim) * 0.2) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Loss:1494.3602137319758\n",
      "Iter:100 Loss:893.0728420462092\n",
      "Iter:200 Loss:231.14138774194103\n",
      "Iter:300 Loss:32.241151935183964\n",
      "Iter:400 Loss:11.739254938641714\n",
      "Iter:500 Loss:7.0253146694269195\n",
      "Iter:600 Loss:4.4895537812402495\n",
      "Iter:700 Loss:3.3468368383612566\n",
      "Iter:800 Loss:2.7051342359774416\n",
      "Iter:900 Loss:2.4634271044110845\n",
      "Iter:999 Loss:1.9223084161488678"
     ]
    }
   ],
   "source": [
    "for iter in range(iterations):\n",
    "    error = 0\n",
    "\n",
    "    for batch_i in range(int(len(x) / batch_size)):\n",
    "        batch_x = x[(batch_i * batch_size):(batch_i+1)*batch_size]\n",
    "        batch_y = y[(batch_i * batch_size):(batch_i+1)*batch_size]    \n",
    "\n",
    "        layer_0 = batch_x\n",
    "        layer_1 = sigmoid(layer_0.dot(weights_0_1))\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2))\n",
    "        layer_3 = sigmoid(layer_2.dot(weights_2_3))\n",
    "\n",
    "        layer_3_delta = (layer_3 - batch_y) * layer_3  * (1 - layer_3)\n",
    "        layer_2_delta = layer_3_delta.dot(weights_2_3.T) * layer_2 * (1 - layer_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * layer_1 * (1 - layer_1)\n",
    "\n",
    "        weights_0_1 -= layer_0.T.dot(layer_1_delta) * alpha\n",
    "        weights_1_2 -= layer_1.T.dot(layer_2_delta) * alpha\n",
    "        weights_2_3 -= layer_2.T.dot(layer_3_delta) * alpha\n",
    "\n",
    "        error += (np.sum(np.abs(layer_3_delta)))\n",
    "\n",
    "    sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error))\n",
    "    if(iter % 100 == 0):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding abstraction using classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:99 Loss:898.45929675101888\n",
      "Iter:199 Loss:235.78940520837367\n",
      "Iter:299 Loss:33.147677949279566\n",
      "Iter:399 Loss:11.453199285864986\n",
      "Iter:499 Loss:6.5955954129791155\n",
      "Iter:599 Loss:4.5845789050599756\n",
      "Iter:699 Loss:3.4047922367327383\n",
      "Iter:799 Loss:2.7167673546422953\n",
      "Iter:899 Loss:2.2479555452452793\n",
      "Iter:999 Loss:1.9593984959086015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def generate_dataset(output_dim = 8,num_examples=1000):\n",
    "    def int2vec(x,dim=output_dim):\n",
    "        out = np.zeros(dim)\n",
    "        binrep = np.array(list(np.binary_repr(x))).astype('int')\n",
    "        out[-len(binrep):] = binrep\n",
    "        return out\n",
    "\n",
    "    x_left_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    x_right_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    y_int = x_left_int + x_right_int\n",
    "\n",
    "    x = list()\n",
    "    for i in range(len(x_left_int)):\n",
    "        x.append(np.concatenate((int2vec(x_left_int[i]),int2vec(x_right_int[i]))))\n",
    "\n",
    "    y = list()\n",
    "    for i in range(len(y_int)):\n",
    "        y.append(int2vec(y_int[i]))\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (x,y)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_out2deriv(out):\n",
    "    return out * (1 - out)\n",
    "\n",
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim,nonlin,nonlin_deriv):\n",
    "        \n",
    "        self.weights = (np.random.randn(input_dim, output_dim) * 0.2) - 0.1\n",
    "        self.nonlin = nonlin\n",
    "        self.nonlin_deriv = nonlin_deriv\n",
    "    \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "        self.output = self.nonlin(self.input.dot(self.weights))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,output_delta):\n",
    "        self.weight_output_delta = output_delta * self.nonlin_deriv(self.output)\n",
    "        return self.weight_output_delta.dot(self.weights.T)\n",
    "    \n",
    "    def update(self,alpha=0.1):\n",
    "        self.weights -= self.input.T.dot(self.weight_output_delta) * alpha\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "num_examples = 1000\n",
    "output_dim = 12\n",
    "iterations = 1000\n",
    "\n",
    "x,y = generate_dataset(num_examples=num_examples, output_dim = output_dim)\n",
    "\n",
    "batch_size = 10\n",
    "alpha = 0.1\n",
    "\n",
    "input_dim = len(x[0])\n",
    "layer_1_dim = 128\n",
    "layer_2_dim = 64\n",
    "output_dim = len(y[0])\n",
    "\n",
    "layer_1 = Layer(input_dim,layer_1_dim,sigmoid,sigmoid_out2deriv)\n",
    "layer_2 = Layer(layer_1_dim,layer_2_dim,sigmoid,sigmoid_out2deriv)\n",
    "layer_3 = Layer(layer_2_dim, output_dim,sigmoid, sigmoid_out2deriv)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    error = 0\n",
    "\n",
    "    for batch_i in range(int(len(x) / batch_size)):\n",
    "        batch_x = x[(batch_i * batch_size):(batch_i+1)*batch_size]\n",
    "        batch_y = y[(batch_i * batch_size):(batch_i+1)*batch_size]  \n",
    "        \n",
    "        layer_1_out = layer_1.forward(batch_x)\n",
    "        layer_2_out = layer_2.forward(layer_1_out)\n",
    "        layer_3_out = layer_3.forward(layer_2_out)\n",
    "\n",
    "        layer_3_delta = layer_3_out - batch_y\n",
    "        layer_2_delta = layer_3.backward(layer_3_delta)\n",
    "        layer_1_delta = layer_2.backward(layer_2_delta)\n",
    "        layer_1.backward(layer_1_delta)\n",
    "        \n",
    "        layer_1.update()\n",
    "        layer_2.update()\n",
    "        layer_3.update()\n",
    "        \n",
    "        error += (np.sum(np.abs(layer_3_delta * layer_3_out * (1 - layer_3_out))))\n",
    "\n",
    "    sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error))\n",
    "    if(iter % 100 == 99):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:9999 Loss:324.912688061634 Synthetic Loss:1633.273569251093248\n",
      "Iter:19999 Loss:288.3624948825246 Synthetic Loss:1551.51824491084023\n",
      "Iter:29999 Loss:308.72952031620645 Synthetic Loss:1673.9582690116692\n",
      "Iter:39999 Loss:306.22282275121415 Synthetic Loss:1715.6189612697463\n",
      "Iter:49999 Loss:282.5846699563283 Synthetic Loss:1638.65966644290356\n",
      "Iter:59999 Loss:296.21254627922923 Synthetic Loss:1623.8765846372145\n",
      "Iter:69999 Loss:297.85114074283035 Synthetic Loss:1754.8820391151328\n",
      "Iter:79999 Loss:284.0449779075638 Synthetic Loss:1721.13177414968466\n",
      "Iter:89999 Loss:294.26174765226864 Synthetic Loss:1879.8383451977818\n",
      "Iter:99999 Loss:307.2924139034243 Synthetic Loss:2007.08663356645278\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def generate_dataset(output_dim = 8,num_examples=1000):\n",
    "    def int2vec(x,dim=output_dim):\n",
    "        out = np.zeros(dim)\n",
    "        binrep = np.array(list(np.binary_repr(x))).astype('int')\n",
    "        out[-len(binrep):] = binrep\n",
    "        return out\n",
    "\n",
    "    x_left_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    x_right_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    y_int = x_left_int + x_right_int\n",
    "\n",
    "    x = list()\n",
    "    for i in range(len(x_left_int)):\n",
    "        x.append(np.concatenate((int2vec(x_left_int[i]),int2vec(x_right_int[i]))))\n",
    "\n",
    "    y = list()\n",
    "    for i in range(len(y_int)):\n",
    "        y.append(int2vec(y_int[i]))\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (x,y)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_out2deriv(out):\n",
    "    return out * (1 - out)\n",
    "\n",
    "class DNI(object):\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim,nonlin,nonlin_deriv,alpha = 0.1):\n",
    "        \n",
    "        self.weights = (np.random.randn(input_dim, output_dim) * 2) - 1\n",
    "        self.bias = (np.random.randn(output_dim) * 2) - 1\n",
    "        \n",
    "        self.weights_0_1_synthetic_grads = (np.random.randn(output_dim,output_dim) * .0) - .0\n",
    "        self.bias_0_1_synthetic_grads = (np.random.randn(output_dim) * .0) - .0\n",
    "    \n",
    "        self.nonlin = nonlin\n",
    "        self.nonlin_deriv = nonlin_deriv\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward_and_synthetic_update(self,input,update=True):\n",
    "        \n",
    "        self.input = input\n",
    "        self.output = self.nonlin(self.input.dot(self.weights)  + self.bias)\n",
    "        \n",
    "        if(not update):\n",
    "            return self.output\n",
    "        else:\n",
    "            self.synthetic_gradient = (self.output.dot(self.weights_0_1_synthetic_grads) + self.bias_0_1_synthetic_grads)\n",
    "            self.weight_synthetic_gradient = self.synthetic_gradient * self.nonlin_deriv(self.output)\n",
    "        \n",
    "            self.weights -= self.input.T.dot(self.weight_synthetic_gradient) * self.alpha\n",
    "            self.bias -= np.average(self.weight_synthetic_gradient,axis=0) * self.alpha\n",
    "        \n",
    "        return self.weight_synthetic_gradient.dot(self.weights.T), self.output\n",
    "    \n",
    "    def normal_update(self,true_gradient):\n",
    "        grad = true_gradient * self.nonlin_deriv(self.output)\n",
    "        \n",
    "        self.weights -= self.input.T.dot(grad) * self.alpha\n",
    "        self.bias -= np.average(grad,axis=0) * self.alpha\n",
    "        \n",
    "        return grad.dot(self.weights.T)\n",
    "    \n",
    "    def update_synthetic_weights(self,true_gradient):\n",
    "        self.synthetic_gradient_delta = (self.synthetic_gradient - true_gradient)\n",
    "        self.weights_0_1_synthetic_grads -= self.output.T.dot(self.synthetic_gradient_delta) * self.alpha\n",
    "        self.bias_0_1_synthetic_grads -= np.average(self.synthetic_gradient_delta,axis=0) * self.alpha\n",
    "        \n",
    "np.random.seed(1)\n",
    "\n",
    "num_examples = 100\n",
    "output_dim = 8\n",
    "iterations = 100000\n",
    "\n",
    "x,y = generate_dataset(num_examples=num_examples, output_dim = output_dim)\n",
    "\n",
    "batch_size = 10\n",
    "alpha = 0.01\n",
    "\n",
    "input_dim = len(x[0])\n",
    "layer_1_dim = 64\n",
    "layer_2_dim = 32\n",
    "output_dim = len(y[0])\n",
    "\n",
    "layer_1 = DNI(input_dim,layer_1_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_2 = DNI(layer_1_dim,layer_2_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_3 = DNI(layer_2_dim, output_dim,sigmoid, sigmoid_out2deriv,alpha)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    error = 0\n",
    "    synthetic_error = 0\n",
    "    \n",
    "    for batch_i in range(int(len(x) / batch_size)):\n",
    "        batch_x = x[(batch_i * batch_size):(batch_i+1)*batch_size]\n",
    "        batch_y = y[(batch_i * batch_size):(batch_i+1)*batch_size]  \n",
    "        \n",
    "        _, layer_1_out = layer_1.forward_and_synthetic_update(batch_x)\n",
    "        layer_1_delta, layer_2_out = layer_2.forward_and_synthetic_update(layer_1_out)\n",
    "        layer_3_out = layer_3.forward_and_synthetic_update(layer_2_out,False)\n",
    "\n",
    "        layer_3_delta = layer_3_out - batch_y\n",
    "        layer_2_delta = layer_3.normal_update(layer_3_delta)\n",
    "        layer_2.update_synthetic_weights(layer_2_delta)\n",
    "        layer_1.update_synthetic_weights(layer_1_delta)\n",
    "        \n",
    "        error += (np.sum(np.abs(layer_3_delta)))\n",
    "        synthetic_error += (np.sum(np.abs(layer_2_delta - layer_2.synthetic_gradient)))\n",
    "    if(iter % 100 == 99):\n",
    "        sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error) + \" Synthetic Loss:\" + str(synthetic_error))\n",
    "    if(iter % 10000 == 9999):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:99 Loss:150.13629731263936\n",
      "Iter:199 Loss:131.42699328822636\n",
      "Iter:299 Loss:137.16011836796628\n",
      "Iter:399 Loss:21.042128720397116\n",
      "Iter:417 Loss:17.929900992298016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:483 Loss:0.0132657308947348"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_dataset(output_dim = 8,num_examples=1000):\n",
    "    def int2vec(x,dim=output_dim):\n",
    "        out = np.zeros(dim)\n",
    "        binrep = np.array(list(np.binary_repr(x))).astype('int')\n",
    "        out[-len(binrep):] = binrep\n",
    "        return out\n",
    "\n",
    "    x_left_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    x_right_int = (np.random.rand(num_examples) * 2**(output_dim - 1)).astype('int')\n",
    "    y_int = x_left_int + x_right_int\n",
    "\n",
    "    x = list()\n",
    "    for i in range(len(x_left_int)):\n",
    "        x.append(np.concatenate((int2vec(x_left_int[i]),int2vec(x_right_int[i]))))\n",
    "\n",
    "    y = list()\n",
    "    for i in range(len(y_int)):\n",
    "        y.append(int2vec(y_int[i]))\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (x,y)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_out2deriv(out):\n",
    "    return out * (1 - out)\n",
    "\n",
    "class DNI(object):\n",
    "    \n",
    "    def __init__(self,input_dim, output_dim,nonlin,nonlin_deriv,alpha = 0.1):\n",
    "        \n",
    "        self.weights = (np.random.randn(input_dim, output_dim) * 0.2) - 0.1\n",
    "        self.weights_synthetic_grads = (np.random.randn(output_dim,output_dim) * 0.2) - 0.1\n",
    "        self.nonlin = nonlin\n",
    "        self.nonlin_deriv = nonlin_deriv\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward_and_synthetic_update(self,input):\n",
    "        self.input = input\n",
    "        self.output = self.nonlin(self.input.dot(self.weights))\n",
    "        \n",
    "        self.synthetic_gradient = self.output.dot(self.weights_synthetic_grads)\n",
    "        self.weight_synthetic_gradient = self.synthetic_gradient * self.nonlin_deriv(self.output)\n",
    "        self.weights += self.input.T.dot(self.weight_synthetic_gradient) * self.alpha\n",
    "        \n",
    "        return self.weight_synthetic_gradient.dot(self.weights.T), self.output\n",
    "    \n",
    "    def update_synthetic_weights(self,true_gradient):\n",
    "        self.synthetic_gradient_delta = self.synthetic_gradient - true_gradient\n",
    "        self.weights_synthetic_grads += self.output.T.dot(self.synthetic_gradient_delta) * self.alpha\n",
    "        \n",
    "np.random.seed(1)\n",
    "\n",
    "num_examples = 1000\n",
    "output_dim = 12\n",
    "iterations = 1000\n",
    "\n",
    "x,y = generate_dataset(num_examples=num_examples, output_dim = output_dim)\n",
    "\n",
    "batch_size = 1000\n",
    "alpha = 0.0001\n",
    "\n",
    "input_dim = len(x[0])\n",
    "layer_1_dim = 128\n",
    "layer_2_dim = 64\n",
    "output_dim = len(y[0])\n",
    "\n",
    "layer_1 = DNI(input_dim,layer_1_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_2 = DNI(layer_1_dim,layer_2_dim,sigmoid,sigmoid_out2deriv,alpha)\n",
    "layer_3 = DNI(layer_2_dim, output_dim,sigmoid, sigmoid_out2deriv,alpha)\n",
    "\n",
    "for iter in range(iterations):\n",
    "    error = 0\n",
    "\n",
    "    for batch_i in range(int(len(x) / batch_size)):\n",
    "        batch_x = x[(batch_i * batch_size):(batch_i+1)*batch_size]\n",
    "        batch_y = y[(batch_i * batch_size):(batch_i+1)*batch_size]  \n",
    "        \n",
    "        _, layer_1_out = layer_1.forward_and_synthetic_update(batch_x)\n",
    "        layer_1_delta, layer_2_out = layer_2.forward_and_synthetic_update(layer_1_out)\n",
    "        layer_2_delta, layer_3_out = layer_3.forward_and_synthetic_update(layer_2_out)\n",
    "\n",
    "        layer_3_delta = layer_3_out - batch_y\n",
    "        layer_3.update_synthetic_weights(layer_3_delta)\n",
    "        layer_2.update_synthetic_weights(layer_2_delta)\n",
    "        layer_1.update_synthetic_weights(layer_1_delta)\n",
    "        \n",
    "        error += (np.sum(np.abs(layer_3_delta * layer_3_out * (1 - layer_3_out))))\n",
    "\n",
    "    if(error < 0.1):\n",
    "        sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error))\n",
    "        break       \n",
    "        \n",
    "    sys.stdout.write(\"\\rIter:\" + str(iter) + \" Loss:\" + str(error))\n",
    "    if(iter % 100 == 99):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
